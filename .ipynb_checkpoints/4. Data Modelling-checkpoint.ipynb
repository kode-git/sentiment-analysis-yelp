{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f89bd0d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40bccaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c480058",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a792325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews and parse JSON\n",
    "reviews = pd.read_csv('./data/yelp_academic_base_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7fe835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>businessId</th>\n",
       "      <th>meanStars</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>reviewStars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Stopped in on a busy Friday night. Despite the...</td>\n",
       "      <td>2018-03-04 00:59:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Went there about 1 PM on a Monday.  It wasn't ...</td>\n",
       "      <td>2018-08-14 05:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This was the place the be on Friday Night! If ...</td>\n",
       "      <td>2018-03-17 14:22:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Went to this place with my family over the wee...</td>\n",
       "      <td>2018-04-04 21:16:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Stopped on a midweek afternoon, and so glad th...</td>\n",
       "      <td>2018-04-28 19:17:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               businessId  meanStars  reviewCount  reviewStars  \\\n",
       "0  6iYb2HFDywm3zjuRg0shjw        4.0           86          5.0   \n",
       "1  6iYb2HFDywm3zjuRg0shjw        4.0           86          2.0   \n",
       "2  6iYb2HFDywm3zjuRg0shjw        4.0           86          5.0   \n",
       "3  6iYb2HFDywm3zjuRg0shjw        4.0           86          4.0   \n",
       "4  6iYb2HFDywm3zjuRg0shjw        4.0           86          4.0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  Stopped in on a busy Friday night. Despite the...  2018-03-04 00:59:21  \n",
       "1  Went there about 1 PM on a Monday.  It wasn't ...  2018-08-14 05:22:00  \n",
       "2  This was the place the be on Friday Night! If ...  2018-03-17 14:22:48  \n",
       "3  Went to this place with my family over the wee...  2018-04-04 21:16:50  \n",
       "4  Stopped on a midweek afternoon, and so glad th...  2018-04-28 19:17:04  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711a9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a balanced sample of positive and negative reviews\n",
    "texts =  reviews['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b662a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "reviews_cleaned = reviews.drop(['businessId', 'meanStars', 'reviewCount', 'date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01cfb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our 5 classes into 2 (negative or positive)\n",
    "binstars = [0 if review <= 3.0 else 1 for review in reviews_cleaned['reviewStars']]\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "limit = 100000  # Change this to grow/shrink the dataset\n",
    "neg_pos_counts = [0, 0]\n",
    "for i in range(len(texts)):\n",
    "    polarity = binstars[i]\n",
    "    if neg_pos_counts[polarity] < limit:\n",
    "        balanced_texts.append(texts[i])\n",
    "        balanced_labels.append(binstars[i])\n",
    "        neg_pos_counts[polarity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef99590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 100000, 0: 100000})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(balanced_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbccae30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee4a38",
   "metadata": {},
   "source": [
    "# Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b797c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 4, 2], [1, 3], [3, 1, 2], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5)\n",
    "toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\"]\n",
    "tokenizer.fit_on_texts(toytexts)\n",
    "sequences = tokenizer.texts_to_sequences(toytexts)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c4440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'common': 2, 'the': 3, 'a': 4, 'word': 5, 'so': 6, 'discombobulation': 7, 'not': 8}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df477eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 4 2]\n",
      " [0 0 1 3]\n",
      " [0 3 1 2]\n",
      " [0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences)\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a393a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "data = pad_sequences(sequences, maxlen=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcfebd8",
   "metadata": {},
   "source": [
    "# Build a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a8f873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-I/usr/local/lib/python3.9/site-packages/tensorflow/include',\n",
       " '-D_GLIBCXX_USE_CXX11_ABI=0',\n",
       " '-DEIGEN_MAX_ALIGN_BYTES=64']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow checking with appropriate compiler flags.\n",
    "tf.sysconfig.get_compile_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d69495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9379876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174931d",
   "metadata": {},
   "source": [
    "# Neural Network Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4225d8d",
   "metadata": {},
   "source": [
    "Andiamo ad aggiungere piÃ¹ layer in modo da ottimizzare la rete neurale e addestrare un modello con accuracy maggiore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a567ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27f3b3",
   "metadata": {},
   "source": [
    "# Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e503a120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:38.394486\n",
      "(200000, 639342)\n",
      "0:00:43.187111\n",
      "[0.89995 0.8985 ]\n",
      "0.8992249999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t1 = datetime.now()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
    "classifier = LinearSVC()\n",
    "Xs = vectorizer.fit_transform(balanced_texts)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(Xs.shape)\n",
    "\n",
    "score = cross_val_score(classifier, Xs, balanced_labels, cv=2, n_jobs=-1)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(score)\n",
    "print(sum(score) / len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a25eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6fc83",
   "metadata": {},
   "source": [
    "# Packaging Model and Testing on Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a49c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_958/4065407482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_tokenizer.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yelp_sentiment_model.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the tokenizer and model\n",
    "with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "   pickle.dump(tokenizer, f)\n",
    "model.save(\"yelp_sentiment_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6360d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-19 11:13:39.105549: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-19 11:13:39.803336: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49636674]\n",
      " [0.22827312]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# load the tokenizer and the model\n",
    "with open(\"keras_tokenizer.pickle\", \"rb\") as f:\n",
    "   tokenizer = pickle.load(f)\n",
    "\n",
    "model = load_model(\"yelp_sentiment_model.hdf5\")\n",
    "\n",
    "# replace with the data you want to classify\n",
    "newtexts = [\"It's so bad\", \"Food is delicious\"]\n",
    "\n",
    "# note that we shouldn't call \"fit\" on the tokenizer again\n",
    "sequences = tokenizer.texts_to_sequences(newtexts)\n",
    "data = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# get predictions for each of your new texts\n",
    "predictions = model.predict(data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4a3de59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (/usr/local/lib/python3.9/site-packages/sklearn/externals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_958/1351690236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"svm_classifier.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_vectorizer.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/usr/local/lib/python3.9/site-packages/sklearn/externals/__init__.py)"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "svmClassifier = \"svm_classifier.pickle\"\n",
    "tfidfVectorizer = \"tfidf_vectorizer.pickle\"\n",
    "classifier = pickle.load(open(svmFile, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pickle\")\n",
    "classifier = joblib.load(\"svm_classifier.pickle\")\n",
    "\n",
    "# replace with the data you want to classify\n",
    "newtexts = [\"Your new data\", \"More new data\"]\n",
    "\n",
    "# note that we should call \"transform\" here instead of the \"fit_transform\" from earlier\n",
    "Xs = vectorizer.transform(newtexts)\n",
    "\n",
    "# get predictions for each of your new texts\n",
    "predictions = classifier.predict(Xs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55d1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
