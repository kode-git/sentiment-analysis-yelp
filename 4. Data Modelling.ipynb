{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f89bd0d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bccaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f12ecf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"create\"\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.40.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.9/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/site-packages (from tensorflow) (3.18.0)\n",
      "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.9/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/site-packages (2.6.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.6 (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow-gpu==2.6\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: tensorflow-estimator==2.6.0 in /usr/local/lib/python3.9/site-packages (2.6.0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c480058",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a792325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews and parse JSON\n",
    "reviews = pd.read_csv('./data/yelp_academic_base_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7fe835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>businessId</th>\n",
       "      <th>meanStars</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>reviewStars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Stopped in on a busy Friday night. Despite the...</td>\n",
       "      <td>2018-03-04 00:59:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Went there about 1 PM on a Monday.  It wasn't ...</td>\n",
       "      <td>2018-08-14 05:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This was the place the be on Friday Night! If ...</td>\n",
       "      <td>2018-03-17 14:22:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Went to this place with my family over the wee...</td>\n",
       "      <td>2018-04-04 21:16:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6iYb2HFDywm3zjuRg0shjw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Stopped on a midweek afternoon, and so glad th...</td>\n",
       "      <td>2018-04-28 19:17:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               businessId  meanStars  reviewCount  reviewStars  \\\n",
       "0  6iYb2HFDywm3zjuRg0shjw        4.0           86          5.0   \n",
       "1  6iYb2HFDywm3zjuRg0shjw        4.0           86          2.0   \n",
       "2  6iYb2HFDywm3zjuRg0shjw        4.0           86          5.0   \n",
       "3  6iYb2HFDywm3zjuRg0shjw        4.0           86          4.0   \n",
       "4  6iYb2HFDywm3zjuRg0shjw        4.0           86          4.0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  Stopped in on a busy Friday night. Despite the...  2018-03-04 00:59:21  \n",
       "1  Went there about 1 PM on a Monday.  It wasn't ...  2018-08-14 05:22:00  \n",
       "2  This was the place the be on Friday Night! If ...  2018-03-17 14:22:48  \n",
       "3  Went to this place with my family over the wee...  2018-04-04 21:16:50  \n",
       "4  Stopped on a midweek afternoon, and so glad th...  2018-04-28 19:17:04  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711a9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a balanced sample of positive and negative reviews\n",
    "texts =  reviews['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cfb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our 5 classes into 2 (negative or positive)\n",
    "binstars = [0 if review <= 3.0 else 1 for review in reviews['reviewStars']]\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "limit = 100000  # Change this to grow/shrink the dataset\n",
    "neg_pos_counts = [0, 0]\n",
    "for i in range(len(texts)):\n",
    "    polarity = binstars[i]\n",
    "    if neg_pos_counts[polarity] < limit:\n",
    "        balanced_texts.append(texts[i])\n",
    "        balanced_labels.append(binstars[i])\n",
    "        neg_pos_counts[polarity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef99590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 100000, 0: 100000})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(balanced_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee4a38",
   "metadata": {},
   "source": [
    "# Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b797c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 4, 2], [1, 3], [3, 1, 2], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5)\n",
    "toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\"]\n",
    "tokenizer.fit_on_texts(toytexts)\n",
    "sequences = tokenizer.texts_to_sequences(toytexts)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c4440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'common': 2, 'the': 3, 'a': 4, 'word': 5, 'so': 6, 'discombobulation': 7, 'not': 8}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df477eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 4 2]\n",
      " [0 0 1 3]\n",
      " [0 3 1 2]\n",
      " [0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences)\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a393a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "data = pad_sequences(sequences, maxlen=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcfebd8",
   "metadata": {},
   "source": [
    "# Build a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d69495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9379876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 18:26:03.143987: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8686"
     ]
    }
   ],
   "source": [
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174931d",
   "metadata": {},
   "source": [
    "# Neural Network Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4225d8d",
   "metadata": {},
   "source": [
    "Andiamo ad aggiungere piÃ¹ layer in modo da ottimizzare la rete neurale e addestrare un modello con accuracy maggiore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a567ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_41979/3576775702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27f3b3",
   "metadata": {},
   "source": [
    "# Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e503a120",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_41979/372353506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t1 = datetime.now()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
    "classifier = LinearSVC()\n",
    "Xs = vectorizer.fit_transform(balanced_texts)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(Xs.shape)\n",
    "\n",
    "score = cross_val_score(classifier, Xs, balanced_labels, cv=2, n_jobs=-1)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(score)\n",
    "print(sum(score) / len(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6fc83",
   "metadata": {},
   "source": [
    "# Packaging Model and Testing on Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a49c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_41979/4065407482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# save the tokenizer and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_tokenizer.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m    \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yelp_sentiment_model.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the tokenizer and model\n",
    "with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "   pickle.dump(tokenizer, f)\n",
    "model.save(\"yelp_sentiment_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6360d7d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_41979/3063074678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load the tokenizer and the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# load the tokenizer and the model\n",
    "with open(\"keras_tokenizer.pickle\", \"rb\") as f:\n",
    "   tokenizer = pickle.load(f)\n",
    "\n",
    "model = load_model(\"yelp_sentiment_model.hdf5\")\n",
    "\n",
    "# replace with the data you want to classify\n",
    "newtexts = [\"Your new data\", \"More new data\"]\n",
    "\n",
    "# note that we shouldn't call \"fit\" on the tokenizer again\n",
    "sequences = tokenizer.texts_to_sequences(newtexts)\n",
    "data = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# get predictions for each of your new texts\n",
    "predictions = model.predict(data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a3de59",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2078138953.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/0y/xx8wd2q52rb755q1lgxwycdm0000gn/T/ipykernel_41979/2078138953.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    And to get predictions on new data, we can load our model from disk with:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pickle\")\n",
    "joblib.dump(classifier, \"svm_classifier.pickle\")\n",
    "\n",
    "And to get predictions on new data, we can load our model from disk with:\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pickle\")\n",
    "classifier = joblib.load(\"svm_classifier.pickle\")\n",
    "\n",
    "# replace with the data you want to classify\n",
    "newtexts = [\"Your new data\", \"More new data\"]\n",
    "\n",
    "# note that we should call \"transform\" here instead of the \"fit_transform\" from earlier\n",
    "Xs = vectorizer.transform(newtexts)\n",
    "\n",
    "# get predictions for each of your new texts\n",
    "predictions = classifier.predict(Xs)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
